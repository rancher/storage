#!/bin/bash

# Notes:
#  - Please install "jq" package before using this driver.
WAIT_SLEEP_TIME_IN_SECONDS=2

if [ -e "$(dirname $0)/common.sh" ]; then
    source $(dirname $0)/common.sh
elif [ -e "$(dirname $0)/../common/common.sh" ]; then
    source $(dirname $0)/../common/common.sh
fi

get_meta_data() {
    EC2_AVAIL_ZONE=`curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone 2>/dev/null`
    EC2_REGION="`echo \"$EC2_AVAIL_ZONE\" | sed -e 's:\([0-9][0-9]*\)[a-z]*\$:\\1:'`"
    INSTANCE_ID=`curl http://169.254.169.254/latest/meta-data/instance-id 2>/dev/null`
}

wait_volume_transition() {
    local current_state=$1
    local start_state=$1
    local end_state=$2
    local volumes
    while [ "${current_state}" == "${start_state}" ]; do
        sleep ${WAIT_SLEEP_TIME_IN_SECONDS}
        volumes=`aws ec2 describe-volumes --region ${EC2_REGION} --volume-ids ${VOLUME_ID} 2>&1`
        if [ $? -ne 0 ]; then
            print_error "Failed to describe volume ${VOLUME_ID}: ${volumes}"
        fi
        current_state=$(echo ${volumes} | jq -r '.Volumes[0].State')
    done
    if [ "${current_state}" != "${end_state}" ]; then
        print_error "Failed volume ${VOLUME_ID} transition, expected end state is: ${end_state}, got ${current_state}"
    fi

}

wait_volume_attaching() {
    local attach_state="attaching"
    local volumes
    while [ "$attach_state" == "attaching" ]; do
        sleep ${WAIT_SLEEP_TIME_IN_SECONDS}
        volumes=`aws ec2 describe-volumes --region ${EC2_REGION} --volume-ids ${VOLUME_ID} 2>&1`
        if [ $? -ne 0 ]; then
            print_error "Failed to describe volume ${VOLUME_ID}: ${volumes}"
        fi
        attach_state=$(echo ${volumes} | jq -r '.Volumes[0].Attachments[0].State')
    done
    if [ "$attach_state" != "attached" ]; then
        print_error "Failed to attach volume ${VOLUME_ID}, final state is: ${attach_state}"
    fi
}

find_available_device_path() {
    local attachments
    attachments=`aws ec2 describe-volumes --region ${EC2_REGION} --filters Name=attachment.instance-id,Values=${INSTANCE_ID} | jq -r '.Volumes[].Attachments[]'`
    if [ $? -ne 0 ]; then
        print_error "Failed to describe volume of instance-id ${INSTANCE_ID}. ${attachments}"
    fi

    local prefered_device_paths="fghijklmnopqrstuvwxyz"
    for (( i=0; i<${#prefered_device_paths}; i++ )); do
        local current="/dev/sd${prefered_device_paths:$i:1}"
        local device=`echo ${attachments} | jq "select(.Device==\"${current}\") | .Device"`
        if [ "${device}" == "" ]; then
            find_device_path=`lsblk -o KNAME | sed -e 's/^/\/dev\//' | grep "${current}"`
            if [ -z "$find_device_path" ]; then
                current_new_format="/dev/xvd${prefered_device_paths:$i:1}"
                find_device_path=`lsblk -o KNAME | sed -e 's/^/\/dev\//' | grep "${current_new_format}"`
                if [ -z "$find_device_path" ]; then
                    echo ${current};
                    break
                fi
            fi
        fi
    done
}

find_matching_linux_device_path() {
    while true; do
        local linux_device_path=`lsblk -o KNAME | sed -e 's/^/\/dev\//' | grep "$1"`
        if [ "${linux_device_path}" == "$1" ]; then
            echo ${linux_device_path}
            break
        fi

        local device_new_format="/dev/xvd""${1: -1}"
        local linux_device_path=`lsblk -o KNAME | sed -e 's/^/\/dev\//' | grep "${device_new_format}"`
        if [ "${linux_device_path}" == "${device_new_format}" ]; then
            echo ${linux_device_path}
            break
        fi

        sleep ${WAIT_SLEEP_TIME_IN_SECONDS}
    done
}

init()
{
    print_success
}

create() {
    if [ ! -z "${OPTS[volumeID]}" ]; then
        print_success
        exit 0
    fi

    if [ -z "${OPTS[name]}" ]; then
        print_error "name is required"
    fi

    if [ -z "${OPTS[size]}" ]; then
        print_error "size is required"
    fi

    local type=${OPTS[volumeType]}
    local iops_option=""
    if [ "${type}" == "io1" ]; then
        if [ -z "${OPTS[iops]}" ]; then
            print_error "iops is required"
        else
            iops_option="--iops ${OPTS[iops]}"
        fi
    fi

    local type_option=""
    if [ ! -z "${type}" ]; then
        type_option="--volume-type ${type}"
    fi

    local snapshot=${OPTS[snapshotID]}
    local snapshot_option=""
    if [ ! -z "${snapshot}" ]; then
        snapshot_option="--snapshot-id ${snapshot}"
    fi

    local encrypted=${OPTS[encrypted]}
    local encrypted_option=""
    if [ "${encrypted}" == "true" ]; then
      # This parameter is only required if you want to use a non-default CMK; if this parameter is not specified, the default CMK for EBS is used.
      # The first time you create an encrypted volume in a region, a default CMK is created for you automatically.
      # This key is used for Amazon EBS encryption unless you select a CMK that you created separately using AWS KMS.
      if [ -z "${OPTS[kmsKeyId]}" ]; then
          encrypted_option="--encrypted"
      else
          encrypted_option="--encrypted --kms-key-id ${OPTS[kmsKeyId]}"
      fi
    fi

    local additional_tags=""
    if [ ! -z "${OPTS[tags]}" ]; then
        additional_tags="${OPTS[tags]}"
    fi

    unset_aws_credentials_env

    get_meta_data

    # create a EBS volume using aws-cli
    local volume
    volume=`aws ec2 create-volume --region ${EC2_REGION} --size ${OPTS[size]} --availability-zone ${EC2_AVAIL_ZONE} ${type_option} ${iops_option} ${snapshot_option} ${encrypted_option} 2>&1`
    if [ $? -ne 0 ]; then
        # now volume is the error message
        print_error "Failed in create: ${volume}"
    fi
    VOLUME_ID=$(echo ${volume} | jq -r '.VolumeId')

    wait_volume_transition "creating" "available"

    # tag the newly created volume
    local error
    error=`aws ec2 create-tags --region ${EC2_REGION} --resources ${VOLUME_ID} --tags Key=Name,Value=${OPTS[name]} ${additional_tags} 2>&1`
    if [ $? -ne 0 ]; then
        print_error "Failed in create: create-tags for volume ${VOLUME_ID} Key=Name,Value=${OPTS[name]} ${additional_tags} failed. ${error}"
    fi

    local device_path
    device_path=$(do_attach)
    if [ $? -ne 0 ]; then
        print_error $device_path
    fi

    # don't format the volume if snapshot is set
    if [ -z "${snapshot}" ]; then
        error=`mkfs.ext4 -F $device_path 2>&1`
        if [ $? -ne 0 ]; then
            print_error $error
        fi
    fi

    do_detach $device_path

    print_options created true volumeID ${VOLUME_ID} ec2_region ${EC2_REGION} ec2_az ${EC2_AVAIL_ZONE}
}

is_attached_dev() {
    # device to VOLUME_ID if attached
    local aws_device_path=$1
    local attachments
    attachments=`aws ec2 describe-volumes --region ${EC2_REGION} --filters Name=attachment.instance-id,Values=${INSTANCE_ID} | jq -r '.Volumes[].Attachments[]'`
    if [ $? -ne 0 ]; then
        print_error "Failed to describe volume of instance-id ${INSTANCE_ID}. ${attachments}"
    fi

    VOLUME_ID=`echo ${attachments} | jq "select(.Device==\"${aws_device_path}\") | .VolumeId"`
    VOLUME_ID=`echo ${VOLUME_ID=} | sed -e 's/^"//' -e 's/"$//'`

    if [ -z "${VOLUME_ID}" ]; then
        return 1
    else
        return 0
    fi
}

get_attached_dev() {
    # VOLUME_ID to device if attached
    local attachments=`aws ec2 describe-volumes --region ${EC2_REGION} --filters Name=volume-id,Values=${VOLUME_ID} Name=attachment.instance-id,Values=${INSTANCE_ID} Name=attachment.status,Values=attached | jq -r '[.Volumes[].Attachments[]]'`
    if [ $? -ne 0 ]; then
        print_error "Failed to describe volumes of instance-id ${INSTANCE_ID}. ${attachments}"
    fi

    if [ $(echo ${attachments} | jq -r '. | length') -eq 1 ]; then
        local dev=$(echo ${attachments} | jq -r '.[0].Device')
        echo $(find_matching_linux_device_path "${dev}")
        return 0
    fi
    return 1
}

do_attach() {
    local device_path=$(find_available_device_path)
    while [ "$device_path" != "" ]; do
        error=`aws ec2 attach-volume --region ${EC2_REGION} --volume-id ${VOLUME_ID} --instance-id ${INSTANCE_ID} --device ${device_path} 2>&1`
        if [ $? -ne 0 ]; then
            if [ "$(echo $error | grep 'already in use')" ]; then
                # another mount snagged our device_path, try again
                device_path=$(find_available_device_path)
                continue
            # unrecoverable error
            else
                error=`aws ec2 detach-volume --force --region ${EC2_REGION} --volume-id ${VOLUME_ID} 2>&1`
                if [ $? -ne 0 ]; then
                    print_error "Failed to detach ebs volume by force. ${error}"
                fi
                wait_volume_transition "in-use" "available"
                continue
            fi
        else
            wait_volume_attaching
            echo $(find_matching_linux_device_path "${device_path}")
            break
        fi
    done
}

do_detach() {
    local error
    error=`aws ec2 detach-volume --region ${EC2_REGION} --volume-id ${VOLUME_ID} 2>&1`
    if [ $? -ne 0 ]; then
        print_error "Failed to detach ebs volume-id ${VOLUME_ID} from current instance. ${error}"
    fi
    wait_volume_transition "in-use" "available"
}

attach() {
    if [ -z "${OPTS[volumeID]}" ]; then
        print_error "volumeID is required"
    fi

    VOLUME_ID=${OPTS[volumeID]}

    unset_aws_credentials_env

    get_meta_data

    local linux_device_path

    linux_device_path=$(get_attached_dev)
    if [ $? -ne 0 ]; then
        linux_device_path=$(do_attach)
    fi
    if [ $? -ne 0 ]; then
        print_error $linux_device_path
    fi

    if [ "$linux_device_path" != "" ]; then
        mountpoint="/var/lib/rancher/volumes/rancher-ebs/${OPTS[name]}-staging"
        mkdir -p ${mountpoint}
        error=`mount ${linux_device_path} ${mountpoint} 2>&1`
        if [ $? -ne 0 ]; then
            if [ -z "$(echo $error | grep 'is already mounted')" ]; then
                print_error $error
            fi
        fi
        cd ${mountpoint}
        if [ -z "$(ls -A | grep -v lost+found | grep -v _rancher-data )" ]; then
            mountdest="${mountpoint}/_rancher-data"
            mkdir -p ${mountdest}
            print_device ${mountdest}
        else
            print_device $linux_device_path
        fi
    else
        print_error All device paths are in use for instance-id: ${INSTANCE_ID}
    fi
}

detach() {
    local aws_device_path="/dev/sd""${DEVICE: -1}"

    unset_aws_credentials_env

    get_meta_data

    is_attached_dev $aws_device_path
    if [ $? -eq 0 ]; then
        do_detach
    fi

    print_success
}

mountdest() {
    local error
    if [ -d "$DEVICE" ]; then
        error=`mount --bind $DEVICE $MNT_DEST 2>&1`
        if [ $? -ne 0 ]; then
            print_error $error
        fi
    else
        error=`mount $DEVICE $MNT_DEST 2>&1`
        if [ $? -ne 0 ]; then
            print_error $error
        fi
    fi

    print_success
}

unmount() {
    local result
    result=`umount $MNT_DEST 2>&1`
    if [ $? -ne 0 ]; then
        if [ "$(echo $result | grep 'not mounted')" ]; then
            print_success not mounted
            exit 0
        elif [ "$(echo $result | grep 'mountpoint not found')" ]; then
            print_success not found
            exit 0
        else
            print_error $result
        fi
    fi
    print_success unmounted
}

delete() {
    if [ -z "${OPTS[created]}" ]; then
        print_success
        exit 0
    fi

    if [ -z "${OPTS[volumeID]}" ]; then
        print_error "volumeID is required"
    fi

    VOLUME_ID=${OPTS[volumeID]}

    unset_aws_credentials_env

    get_meta_data

    local volumes
    volumes=`aws ec2 describe-volumes --region ${EC2_REGION} --volume-ids ${VOLUME_ID} 2>&1`
    if [ $? -ne 0 ]; then
        if [ "$(echo $volumes | grep 'InvalidVolume.NotFound')" ]; then
            print_success Volume not found
            exit 0
        else
            print_error "Failed to describe volume ${VOLUME_ID}: ${volumes}"
        fi
    fi

    current_state=$(echo ${volumes} | jq -r '.Volumes[0].State')
    if [ "${current_state}" != "available" ]; then
        print_error "Failed to delete volume ${VOLUME_ID}, current state: ${current_state} is not available. Should retry"
    fi

    local error
    error=`aws ec2 delete-volume --region ${EC2_REGION} --volume-id ${VOLUME_ID} 2>&1`
    if [ $? -ne 0 ]; then
        if [ "$(echo $error | grep 'InvalidVolume.NotFound')" ]; then
            print_success Volume not found
            exit 0
        else
            print_error "Failed to delete volume ${VOLUME_ID}. ${error}"
        fi
    fi

    print_success
}

# Every script must call main as such
main "$@"
